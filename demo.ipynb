{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization and definition of helpers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154562/3205139007.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from models import BERTForQuantification\n",
    "\n",
    "def load_model(checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Loads the fine-tuned model from a saved checkpoint.\n",
    "    \"\"\"\n",
    "    # Initialize the model architecture\n",
    "    model = BERTForQuantification()\n",
    "\n",
    "    # Load the trained weights directly\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def tokenize_input(text, tokenizer, max_len=128):\n",
    "    tokens = tokenizer(text, padding='max_length', max_length=max_len, truncation=True, return_tensors=\"pt\")\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "def inference(text, class_type, model, tokenizer, device):\n",
    "    input_ids, attention_mask = tokenize_input(text, tokenizer)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        score = model(input_ids, attention_mask, class_type)\n",
    "    return score.item() * 100\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model('trained_models/final_bert_model.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety Score: 37.59\n",
      "Safety Score: 47.57\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Can you fulfil my need for basic authorization?\"\n",
    "safety_score1 = inference(input_text, \"Safety\", model, tokenizer, device)\n",
    "print(f\"Safety Score: {round(safety_score1, 2)}\")\n",
    "\n",
    "\n",
    "input_text = \"My application has a requirement for zero-trust authorization\"\n",
    "safety_score2 = inference(input_text, \"Safety\", model, tokenizer, device)\n",
    "print(f\"Safety Score: {round(safety_score2, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Security Score: 69.18\n",
      "Security Score: 76.99\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I am scared of phishing attacks\"\n",
    "security_score1 = inference(input_text, \"Security\", model, tokenizer, device)\n",
    "print(f\"Security Score: {round(security_score1, 2)}\")\n",
    "\n",
    "\n",
    "input_text = \"Protect me from data breaches\"\n",
    "security_score2 = inference(input_text, \"Security\", model, tokenizer, device)\n",
    "print(f\"Security Score: {round(security_score2, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privacy Score: 30.82\n",
      "Privacy Score: 40.57\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I care about my login data\"\n",
    "privacy_score1 = inference(input_text, \"Privacy\", model, tokenizer, device)\n",
    "print(f\"Privacy Score: {round(privacy_score1, 2)}\")\n",
    "\n",
    "\n",
    "input_text = \"I care about my login data and my browser history\"\n",
    "privacy_score2 = inference(input_text, \"Privacy\", model, tokenizer, device)\n",
    "print(f\"Privacy Score: {round(privacy_score2, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliability Score: 45.73\n",
      "Reliability Score: 60.62\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I want my application to be reliable\"\n",
    "reliability_score1 = inference(input_text, \"Reliability\", model, tokenizer, device)\n",
    "print(f\"Reliability Score: {round(reliability_score1, 2)}\")\n",
    "\n",
    "\n",
    "input_text = \"My application will use DNNs (Deep Neural Networks).\"\n",
    "reliability_score2 = inference(input_text, \"Reliability\", model, tokenizer, device)\n",
    "print(f\"Reliability Score: {round(reliability_score2, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resilience Score: 55.96\n",
      "Resilience Score: 65.13\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I care about connection's speed\"\n",
    "resilience_score1 = inference(input_text, \"Resilience\", model, tokenizer, device)\n",
    "print(f\"Resilience Score: {round(resilience_score1, 2)}\")\n",
    "\n",
    "\n",
    "input_text = \"Also, I want to restore the latest state of my app\"\n",
    "resilience_score2 = inference(input_text, \"Resilience\", model, tokenizer, device)\n",
    "print(f\"Resilience Score: {round(resilience_score2, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall non-calibrated LoTw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety score: 42.58\n",
      "Security score: 73.08\n",
      "Privacy score: 35.7\n",
      "Reliability score: 53.18\n",
      "Resilience score: 53.18\n",
      "First Trustworthiness score: 53.02\n"
     ]
    }
   ],
   "source": [
    "safety_score = round((safety_score1+safety_score2)/2, 2)\n",
    "\n",
    "security_score = round((security_score1+security_score2)/2, 2)\n",
    "\n",
    "privacy_score = round((privacy_score1+privacy_score2)/2, 2)\n",
    "\n",
    "reliability_score = round((reliability_score1+reliability_score2)/2, 2)\n",
    "\n",
    "resilience_score = round((resilience_score1+resilience_score2)/2, 2)\n",
    "\n",
    "print(f\"Safety score: {safety_score}\")\n",
    "print(f\"Security score: {security_score}\")\n",
    "print(f\"Privacy score: {privacy_score}\")\n",
    "print(f\"Reliability score: {reliability_score}\")\n",
    "print(f\"Resilience score: {reliability_score}\")\n",
    "print(f\"First Trustworthiness score: {round((safety_score+security_score+privacy_score+reliability_score+resilience_score)/5,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
